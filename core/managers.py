from typing import Dict, List, Type, Tuple, Optional, Union
from collections import defaultdict
# from dataclasses import fields, is_dataclass # No longer needed as we've moved to Pydantic
import daft
from daft import col, lit, DataType, Schema, DataFrame
from daft.expressions import Expression
import daft.expressions as F
import pyarrow as pa # Import pyarrow

# Import from our new structure
from .base import Component, EntityType
from .store import EcsComponentStore # Needs the store to interact with

# --- Query Interface ---
class EcsQueryInterface:
    """
    Provides read-only access to the *latest active* ECS state,
    derived from the historical ComponentStore.
    """
    def __init__(self, component_store: EcsComponentStore):
        self._store = component_store

    def get_component(self, component_type: Type[Component]) -> daft.DataFrame:
        """
        Gets the latest active state for a specific component type.
        """
        df = self._store.get_component(component_type)

        # Prep Sorted Bucket Merge Join  (THE COOLEST DATA ENGINEERING HACK EVER)
        df = df.where(col("is_active")).exclude("is_active") \
            .sort(col("step")) \
            .repartition(col("step"))
        
        return df
    
    def get_combined_state_history(self, *component_types: Type[Component]) -> List[daft.DataFrame]:
        """
        Gets the latest active state for all specified component types.

        Joins component DataFrames on entity_id, step, and is_active to maintain proper historical state
        and ensure we only join active records from the same step together.
        """
        if not component_types:
            component_types = self._store.components.keys()

        df = self.get_component(component_types[0])
        for component_type in component_types[1:]:
            new_df = self.get_component(component_type)
            df = df.join(
                new_df, 
                on=["entity_id", "step"],
                prefix=f"{component_type.__name__.lower()}.",
                how="inner",
                strategy="sort_merge"
            )

        return df

    def get_latest_active_state_from_step(self, *component_types: Type[Component], step: Optional[int] = None) -> daft.DataFrame:
        """
        Gets the latest active state for all specified component types from a specific step.
        """
        df = self.get_combined_state_history(*component_types)
        
        # Filter to only include steps up to and including the specified step
        if step is not None:
            df = df.where(col("step") <= step)
        
        # Calculate the latest step for each entity (Potentially non-uniform latest step)
        latest_steps = df.groupby("entity_id").agg(F.max(col("step")).alias("latest_step"))

        # Join back to get the full row for the latest step
        latest_df = df.join(
            latest_steps,
            left_on=["entity_id", "step"],
            right_on=["entity_id", "latest_step"],
            how="inner",
            strategy="sort_merge"
        )

        # Select only original columns defined in the schema (excluding 'latest_step')
        final_df = latest_df.select(*[col(name) for name in df.column_names()])
        
        return final_df

    def get_components(self, *component_types: Type[Component], step: Optional[int] = None) -> daft.DataFrame:
        """
        Gets the latest active state for all specified component types.
        """
        return self.get_latest_active_state_from_step(*component_types, step=step)


# --- Update Manager ---
class EcsUpdateManager:
    """
    Manages pending component updates generated by processors during a step.
    Updates are queued as Daft DataFrame plans (containing entity_id + component fields)
    and committed by adding step and activation info before appending to the ComponentStore's history.
    """
    def __init__(self, component_store: EcsComponentStore):
        self._store = component_store
        # ComponentType -> List of DataFrame plans representing updates for that type
        self._pending_updates: Dict[Type[Component], List[daft.DataFrame]] = defaultdict(list)

    def clear_pending_updates(self):
        """Clears all queued updates. Called at the start of each step."""
        self._pending_updates.clear()
        # print("UpdateManager: Cleared pending updates.")

    def add_update(self, components: Union[Type[Component], List[Type[Component]]], update_df: daft.DataFrame):
        """
        Queues a DataFrame plan representing component updates (additions or modifications).
        The input `update_df` should contain 'entity_id' and the component-specific fields.
        It does NOT need 'step' or 'is_active' at this stage.
        """

        # Basic check: Ensure entity_id exists in the update
        if "entity_id" not in update_df.column_names:
            print(f"ERROR: UpdateManager update_df missing required 'entity_id' column. Update ignored.")
            return

        # No strict schema validation here. We rely on the commit phase to cast correctly.
        # Just queue the provided DataFrame plan.
        self._pending_updates[components].append(update_df)
        print(f"UpdateManager: Queued update for {components}. Current keys: {list(self._pending_updates.keys())}")
        # print(f"UpdateManager: Queued raw update plan for {component_type.__name__}")


    def commit_updates(self, current_step: int):
        """
        Aggregates pending updates, adds step and activation info, ensures schema
        alignment, and instructs the ComponentStore to append them to the historical state.

        Args:
            current_step: The current simulation step number.
        """
        print(f"UpdateManager: Committing updates for step {current_step}...")
        if not self._pending_updates:
            print("  No pending updates to commit.")
            return

        num_committed_types = 0
        print(f"  UpdateManager: Pending update keys: {list(self._pending_updates.keys())}")
        for components, update_list in self._pending_updates.items():
            print(f"  UpdateManager: Processing updates for {components}...")
            if not update_list:
                print(f"    Skipping {components}: No updates in list.")
                continue

            # 1. Aggregate all update plans for this component type
            aggregated_updates_df: Optional[daft.DataFrame] = None
            if len(update_list) == 1:
                aggregated_updates_df = update_list[0]
            else:
                try:
                    # Use the top-level daft.concat function with a list
                    aggregated_updates_df = daft.concat(dataframes=update_list)
                except Exception as e:
                     print(f"ERROR: UpdateManager failed to concat updates for {components} at step {current_step}. Skipping type. Error: {e}")
                     # Potentially inspect individual update_list schemas here
                     continue # Skip to next component type

            # 2. --- Handle Duplicates (Keep Last) ---
            # If multiple updates for the same entity exist in this step, keep only the last one.
            # Sort by an implicit order (if concat preserves it) or add a temporary index?
            # Daft's concat doesn't guarantee order preservation across partitions.
            # A common strategy is drop_duplicates on entity_id, keeping the 'last' row.
            # However, defining 'last' without explicit ordering is ambiguous.
            # For simplicity now, let's assume the store handles appending, and queries
            # (`get_latest_...`) correctly find the highest step number.
            # If strict "last update within a step" is needed, more complex logic is required here
            # (e.g., adding row numbers before concat, then grouping/filtering).
            final_update_plan = aggregated_updates_df # Keep as is for now

            # 3. Add step and is_active columns
            try:
                updates_with_meta = final_update_plan.with_column(
                    "step", lit(current_step).cast(DataType.int64())
                ).with_column(
                    "is_active", lit(True).cast(DataType.bool())
                )
            except Exception as e:
                print(f"ERROR: UpdateManager failed adding step/is_active for {components} at step {current_step}. Skipping type. Error: {e}")
                continue

            # 4. --- Trigger the actual application in the component store ---
            # The store's apply_updates now expects the step number as an argument
            # and handles schema casting internally before concat.
            try:
                # print(f"  - Submitting processed update plan for {component_type.__name__} to store for step {current_step}.")
                self._store.apply_updates(components, updates_with_meta, current_step)
                num_committed_types += 1
            except Exception as e:
                # Catch errors during the store's apply_updates if needed
                print(f"ERROR: Store failed applying updates for {components} at step {current_step}. Error: {e}")
                # Potentially log schemas involved:
                # print("Schema being sent to store:")
                # updates_with_meta.print_schema()
                # store_schema = self._store.get_component_schema(component_type)
                # if store_schema: store_schema.print_schema()

        print(f"UpdateManager: Submitted updates for {num_committed_types} component types to the store for step {current_step}.")
